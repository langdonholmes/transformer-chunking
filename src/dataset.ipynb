{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07958483-1b50-4a56-94d6-e2ad18313bfa",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7240245-3811-442f-8a1f-6f0b04b56314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c3e5e0-5cb9-4f12-a085-4a78f030a31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "ds = load_dataset('bookcorpus', split='train[:5000]')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efd139-c5f9-47ff-822b-8ed45e484519",
   "metadata": {},
   "source": [
    "## Indexing the Model State Array with PyTorch\n",
    "\n",
    "hidden_states is a tuple of tensors/arrays. \n",
    "\n",
    "There is one tensor/array for each embedding in the network:\n",
    "hidden_states[i] == hidden states at i<sup>th</sup> layer of network.\n",
    "\n",
    "Each of these tensors/arrays has the following shape:\n",
    "hidden_states[i].shape == [num_examples, sequence_length, embedding_size]  \n",
    "\n",
    "To get the 13 different embeddings for a single token, we loop over the layers of hidden_states. We collect an example sentence isent, a token within that sentence itok, and all 768 scalar values in the embedding matrix using the colon indexer.\n",
    "\n",
    "For this example, we have:\n",
    " - 1 sentence\n",
    " - 7 is the sequence length\n",
    " - 13 layers, which is one input embedding *x*_i_ + 12 encoder blocks\n",
    " - 768 is the embedding size for BERT embeddings\n",
    " \n",
    "The result is a matrix of shpae [1 x 7 x 13 x 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe5a321-726f-4150-88c1-62ac591ceaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging, AutoModel, AutoTokenizer\n",
    "logging.set_verbosity_error()\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f5e1fc3-4947-4ae3-aafe-3636caa5f432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the max sequence length?\n",
    "max([len(tokenizer(s)['input_ids']) for s in ds['text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42f31a-9da1-4082-847e-cf06e938386e",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927c61b4-0657-4b48-8ce0-d6e181af14ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 66, 13, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_array = torch.zeros((5000, 66, 13, 768))\n",
    "embed_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "76f4ca5d-a029-440c-a57b-86734baa737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:40<00:00, 49.99it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_list = []\n",
    "tok_ids = []\n",
    "\n",
    "for i, sample in enumerate(tqdm(ds['text'])):\n",
    "    batch_idx = 0 # one sample at a time, no batching.\n",
    "    \n",
    "    inputs = tokenizer(sample, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden_states = model(**inputs,\n",
    "                              output_hidden_states=True)['hidden_states']\n",
    "        \n",
    "        embed_list.append(torch.stack(hidden_states, dim=2))\n",
    "        tok_ids.append(inputs.input_ids[batch_idx].tolist())\n",
    "        # seq_length = hidden_states[batch_idx].size(1)\n",
    "        # embed_array[i:i+1, :seq_length, : , :] = torch.stack(hidden_states, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344eba6-634e-42dc-b847-c86eb2e3e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embed_list, '../data/bookcorpus_embeddings_0_5000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e056a22-049d-4b9a-8824-10d244cb73a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_array = torch.load('../data/bookcorpus_embeddings_0_5000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c8bb0e3d-063b-47df-97c0-7d0b7cfade7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_list = torch.load('test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d567ead2-c9bf-4876-b68d-6b3f643db59a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5099 is out of bounds for dimension 0 with size 5000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membed_array\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5099\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5099 is out of bounds for dimension 0 with size 5000"
     ]
    }
   ],
   "source": [
    "embed_array.[5099]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42f567-1609-4c16-a276-554f00211f3a",
   "metadata": {},
   "source": [
    "## BERT lookup embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef374c6d-9b66-407e-8809-03794ff3535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeds = torch.Tensor(model.embeddings.word_embeddings.weight)\n",
    "torch.save(bert_embeds, '../data/bert_lookup_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31a3dc5b-5374-4328-8679-eaed659f639f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
       "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
       "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
       "        ...,\n",
       "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
       "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
       "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
       "       grad_fn=<AliasBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "de21c75f-d9a1-4070-af60-545ac7f3f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_lookup(lookup_embeds, test_vector, topk=3):\n",
    "    dist = torch.norm(lookup_embeds - test_vector, dim=1, p=None)\n",
    "    knn = dist.topk(topk, largest=False)\n",
    "    dist_ind_pairs = list(zip(knn.values.round(decimals=2).tolist(),\n",
    "                              [tokenizer.decode([x]) for x in knn.indices.tolist()]\n",
    "                             ))\n",
    "    return dist_ind_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0750f37e-d776-4260-85e6-76acb775f9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14.100000381469727, '[CLS]'),\n",
       " (14.119999885559082, 'be'),\n",
       " (14.149999618530273, 'is')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbor_lookup(bert_embeds, embed_array[0][5][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "18f7c2c5-4092-47bb-a42b-f85a6ddb0a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but just one look at a minion sent him practically catatonic .'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ea7b125-31ce-4007-878e-ec38bb7c3d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be been being'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2022, 2042, 2108])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bfadd4b1-daa2-4f8f-8da2-2e9888a634e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('what to do', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "96c86211-6aad-4435-b28d-296b1bfb4939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2054, 2000, 2079, 102]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers]",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
