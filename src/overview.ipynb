{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786fa14d-9089-4ad9-b8b9-a71ed66a526a",
   "metadata": {},
   "source": [
    "# An information-theoretic probing of wordpiece tokens in BERT\n",
    "\n",
    "This is a methods exploration of an approach proposed in Bruenner et al. (2020), \"On Identifiability in Transformers.\" Neural networks are often described as black boxes because, compared to classic statistical methods such as regressions, they can be difficult to interpret. This problem is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51dcd65-044d-4555-bce8-0cb56f343e24",
   "metadata": {},
   "source": [
    "## Lexical chunking\n",
    "\n",
    "Bybee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0230e1-e052-4599-b5f9-ac9920db0ab2",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are a type of neural network characterized by multi-headed self-attention. They are typically extremely large, having millions or even hundreds of billions of parameters. They were first proposed for the purpose of neural machine translation. Since that time, they have become state-of-the-art for natural language and image processing.\n",
    "\n",
    "We will take a quick look at multi-headed self-attention, but it is worth noting that most of the parameters in a transformer model are devoted to run-of-the-mill fully connected feed forward layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395659f4-c492-4ab0-a382-b0e5d9213990",
   "metadata": {},
   "source": [
    "### Multi-headed self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cfe3f-95ec-4103-affa-c1b30696ce90",
   "metadata": {},
   "source": [
    "### Size matters\n",
    "\n",
    "Large neural networks exhibit emergent behavior that simply cannot be studied with smaller networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0882f25-d3ac-4721-a885-9226838b89c8",
   "metadata": {},
   "source": [
    "## Question\n",
    "Do more word-y words retain more token-specific information through the layers of the transformer blocks? Do compositional words like \"got to\" retain less token-specific information as they move through the transformer blocks?\n",
    "\n",
    "Does the predictability of a token affect how much of its initial token embed is retained?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f18b63-b7f1-483e-9b49-16e61bb759da",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "### Data\n",
    "\n",
    "#### BookCorpus\n",
    "\n",
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755d7c01-1aef-4c6b-8691-3a35ba0f7490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging, AutoModel, AutoTokenizer\n",
    "logging.set_verbosity_error()\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "s = 'This is an example.'\n",
    "s = tokenizer(s, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden_states = model(**s, output_hidden_states=True)['hidden_states']\n",
    "\n",
    "isent, itok = 0, 0\n",
    "\n",
    "for layer in hidden_states:\n",
    "    assert len(layer[isent, itok, : ]) == 768\n",
    "\n",
    "torch.stack(hidden_states, dim=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97f916-fde0-4d80-8b20-abbea5eeaf9d",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Train a multilayer perceptron (MLP) to reverse engineer token embeddings from transformer layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad837321-5e14-4f0c-b394-465084c46bb6",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "\n",
    "Nearest-neighbor lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e5a51-e615-4e46-804a-fbb2d45350ae",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1a186-9e31-4313-896f-d5fe777147da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(font='Liberation Serif',\n",
    "              rc={'figure.figsize': (7.5,3.75),\n",
    "                  'font.size': 11,\n",
    "                 })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5dec4-20b0-43ff-8a63-e0593f2da292",
   "metadata": {},
   "source": [
    "## Limitations/ TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
