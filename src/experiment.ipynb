{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7f5bd8-7243-45a9-b3b8-eb0bb879ed25",
   "metadata": {},
   "source": [
    "# An information-theoretic probing of wordpiece tokens in BERT\n",
    "\n",
    "This is a methods exploration of an approach proposed in Bruenner et al. (2020), \"On Identifiability in Transformers.\" Neural networks are often described as black boxes because, compared to classic statistical methods such as regressions, they can be difficult to interpret. This problem is \n",
    "\n",
    "## Lexical chunking\n",
    "\n",
    "Bybee\n",
    "\n",
    "## Transformers\n",
    "\n",
    "Transformers are a type of neural network characterized by multi-headed self-attention. They are typically extremely large, having millions or even hundreds of billions of parameters. They were first proposed for the purpose of neural machine translation. Since that time, they have become state-of-the-art for natural language and image processing.\n",
    "\n",
    "We will take a quick look at multi-headed self-attention, but it is worth noting that most of the parameters in a transformer model are devoted to run-of-the-mill fully connected feed forward layers.\n",
    "\n",
    "### Multi-headed self-attention\n",
    "\n",
    "## Question\n",
    "Do more word-y words retain more token-specific information through the layers of the transformer blocks? Do compositional words like \"got to\" retain less token-specific information as they move through the transformer blocks?\n",
    "\n",
    "Does the predictability of a token affect how much of its initial token embed is retained?\n",
    "\n",
    "## Method\n",
    "\n",
    "### Data\n",
    "\n",
    "#### BookCorpus\n",
    "\n",
    "#### BERT\n",
    "\n",
    "### Model\n",
    "\n",
    "Train a multilayer perceptron (MLP) to reverse engineer token embeddings from transformer layers.\n",
    "\n",
    "### Statistical Analysis\n",
    "\n",
    "Nearest-neighbor lookup\n",
    "\n",
    "## Results\n",
    "\n",
    "## Limitations/ TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7815d63e-c418-45b3-a7dd-b856f44e8adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm #tqdm.auto is still bugging out. we can use the CLI version.\n",
    "\n",
    "# Huggingface\n",
    "from transformers import logging, AutoModel, AutoTokenizer\n",
    "logging.set_verbosity_error()\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3536a87-9ad4-45e2-8871-752ee785dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'This is an example.'\n",
    "s = tokenizer(s, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48177552-25d6-4494-99fc-3321fae9f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden_states = model(**s, output_hidden_states=True)['hidden_states']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1033e-508b-4ee3-b942-56920da69594",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Indexing the Model State Array with PyTorch\n",
    "\n",
    "hidden_states is a tuple of tensors/arrays. \n",
    "\n",
    "There is one tensor/array for each embedding in the network:\n",
    "hidden_states[i] == hidden states at i<sup>th</sup> layer of network.\n",
    "\n",
    "Each of these tensors/arrays has the following shape:\n",
    "hidden_states[i].shape == [num_examples, sequence_length, embedding_size]  \n",
    "\n",
    "To get the 13 different embeddings for a single token, we loop over the layers of hidden_states. We collect an example sentence isent, a token within that sentence itok, and all 768 scalar values in the embedding matrix using the colon indexer.\n",
    "\n",
    "For this example, we have:\n",
    " - 1 sentence\n",
    " - 7 is the sequence length\n",
    " - 13 layers, which is one input embedding *x*_i_ + 12 encoder blocks\n",
    " - 768 is the embedding size for BERT embeddings\n",
    " \n",
    "The result is a matrix of shpae [1 x 7 x 13 x 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07e8840-a44f-4038-a3f1-c01dadf3b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "isent = 0\n",
    "itok = 0\n",
    "\n",
    "for layer in hidden_states:\n",
    "    assert len(layer[isent, itok, : ]) == 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822c7a9-10d2-46d4-bfce-f6224f7fc120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 13, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(hidden_states, dim=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73419fa9-0c0f-49d6-87e9-dcda375b84a8",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "> A large corpus of books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf291c5-a4ee-4ea9-b33f-a6e2db9de435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "ds = load_dataset('bookcorpus', split='train[:5000]')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466601f9-62e7-4f28-b4e4-ab6acb4d57fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the max sequence length?\n",
    "max([len(tokenizer(s)['input_ids']) for s in ds['text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7d8dc-92a4-4513-9526-707674230ab9",
   "metadata": {},
   "source": [
    "## Process data\n",
    "\n",
    "Construct an embed_array of shape [num_sentences x 66 x 13 x 768]\n",
    "\n",
    "Where Y is the lookup embed stored at embed_array[ : , : , 0 , : ]  \n",
    "And X is any of the 0 < i <= 13 intermediate embeds at embed_array[ : , : , i , : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fb770-41ee-40e7-9331-e0bac46c9b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 512, 13, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embed_array = np.zeros((5000, 66, 13, 768))\n",
    "embed_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abcfd3-aab9-42f3-a97c-34171877e34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 66, 13, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_array = torch.zeros((5000, 66, 13, 768))\n",
    "embed_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb2948-e1e4-48b3-be53-2727ce8ea01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:31<00:00, 54.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(tqdm(ds['text'])):\n",
    "    inputs = tokenizer(sample, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        hidden_states = model(**inputs,\n",
    "                              output_hidden_states=True)['hidden_states']\n",
    "        seq_length = hidden_states[0].shape[1]\n",
    "        embed_array[i:i+1, :seq_length, : , :] = torch.stack(hidden_states, dim=2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1481c0-f8a5-4a06-828b-98f76b5b8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embed_array, '../data/bookcorpus_embeddings_0_5000.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0480e038-7522-448f-bf78-b09a3bed6103",
   "metadata": {},
   "source": [
    "## Data Splits\n",
    "\n",
    "We will need an array of example embeddings paired with target lookup embeddings. We can ignore most of the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644e176-fd55-4415-91df-76ec23db6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_array = torch.load('../data/bookcorpus_embeddings_0_5000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203e121-7387-4ab1-830c-f1d304ae77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(array, layer):\n",
    "    assert isinstance(layer, int)\n",
    "    array[:, :, ]\n",
    "    X = torch.flatten(embed_array[:, :, layer, :], end_dim=1) # generated embeddings in specified layer\n",
    "    y = torch.flatten(embed_array[:, :, 0, :], end_dim=1) # lookup embedding in first layer\n",
    "    print(X.shape, y.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd915c-61d9-494c-863c-cf9f88e02a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([330000, 768]) torch.Size([330000, 768])\n"
     ]
    }
   ],
   "source": [
    "X, y = get_X_y(embed_array, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfc4e1-a373-47ea-b1c0-d4a182d44abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91814b40-b57f-478c-8fbd-04916427594e",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "Multilayer Perceptron\n",
    "\n",
    "> The linear perceptron and MLP are both trained by either minimizing the L2 or cosine distance loss using the ADAM optimizer (Kingma & Ba, 2015) with a learning rate of α = 0.0001, β1 = 0.9 and β2 = 0.999. We use a batch size of 256. We monitor performance on the validation set and stop training if there is no improvement for 20 epochs. The input and output dimension of the models is d = 768; the dimension of the contextual word embeddings. For both models we performed a learning rate search over the values α ∈ [0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001, 0.000003]. The weights are initialized with the Glorot Uniform initializer (Glorot & Bengio, 2010). The MLP has one hidden layer with 1000 neurons and uses the gelu activation function (Hendrycks & Gimpel, 2016), following the feed-forward layers in BERT and GPT. We chose a hidden layer size of 1000 in order to avoid a bottleneck. We experimented with using a larger hidden layer of size 3072 and adding dropout to more closely match the feed-forward layers in BERT. This only resulted in increased training times and we hence deferred from further architecture search. We split the data by sentences into train/validation/test according to a 70/15/15 split. This way of splitting the data ensures that the models have never seen the test sentences (i.e., contexts) during training. In order to get a more robust estimate of performance we perform the experiments in Figure 2a using 10-fold cross validation. The variance, due to the random assignment of sentences to train/validation/test sets, is small, and hence not shown.  \n",
    "> -- <cite>Brunner et al. 2020</cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e736a4-fbc1-4969-a08b-5cadc29ae91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "brunner_param_dict = {\n",
    "    'hidden_layer_sizes': (1000,), # Brunner et al.\n",
    "    'activation': 'relu', # Brunner et al. used 'gelu,' but that is not implemented in sklearn\n",
    "    'solver': 'adam', # Brunner et al.\n",
    "    'alpha': 0.0001, # L2 regularization term, default setting.\n",
    "    'batch_size': '256', # Brunner et al.\n",
    "    'learning_rate': 'constant', # Brunner et al.\n",
    "    'learning_rate_init': 0.0001, # Brunner et al.\n",
    "    'max_iter': 999, # we want to stop after n_iter_no_change, not max_iter, so this is just a high value.\n",
    "    'shuffle': True,\n",
    "    'random_state': random_seed,\n",
    "    'tol': 0.0001, # amount by which performance must improve to reset the n_iter_no_change counter, so this is just a small value.\n",
    "    'verbose': False,\n",
    "    'warm_start': False,\n",
    "    'nesterovs_momentum': True,\n",
    "    'early_stopping': False,\n",
    "    'validation_fraction': 0.1,\n",
    "    'beta_1': 0.9, # Brunner et al.\n",
    "    'beta_2': 0.999, # Brunner et al.\n",
    "    'epsilon': 1e-08, # Default for Adam optimizer\n",
    "    'n_iter_no_change': 20, # Brunner et al.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b3cd14f-3e33-42a8-a8f7-6b9cece35516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenIdentifier(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super(MLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.input_size, self.hidden_size),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(self.hidden_size, self.input_size),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce723490-e664-416a-96f8-89e1e94293bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5dec4-20b0-43ff-8a63-e0593f2da292",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1a186-9e31-4313-896f-d5fe777147da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(font='Liberation Serif',\n",
    "              rc={'figure.figsize': (7.5,3.75),\n",
    "                  'font.size': 11,\n",
    "                 })\n",
    "\n",
    "import pandas as pd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
